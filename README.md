# ğŸŸï¸ Eventscape â€” Event Discovery Platform

A full-stack event discovery platform built with **Next.js 16**, **MongoDB**, and **NextAuth**. It automatically scrapes real event data from the Ticketmaster API on a recurring schedule and presents them through a polished public-facing UI and a secure admin dashboard.

---

## âœ¨ Features

- ğŸ” **Live Event Discovery** â€” Browse real events scraped from Ticketmaster
- ğŸ”„ **Auto-Scraping Engine** â€” Background daemon refreshes data every 6 hours
- â˜ï¸ **Vercel Cron Integration** â€” Serverless auto-scrape via `/api/scrape` on deploy
- ğŸ” **Google OAuth Authentication** â€” Secure login via NextAuth
- ğŸ›¡ï¸ **Admin Dashboard** â€” Manage events and view ticket leads (admin-only)
- ğŸ« **Ticket Lead Capture** â€” Lets users express interest in events
- ğŸ“¦ **MongoDB Atlas** â€” Persistent cloud database via Mongoose

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|---|---|
| Framework | Next.js 16 (App Router) |
| Language | TypeScript |
| Database | MongoDB Atlas + Mongoose |
| Auth | NextAuth v5 (Google OAuth) |
| Styling | Tailwind CSS v4 |
| Animations | Framer Motion |
| Scraping | Ticketmaster Discovery API v2 |
| Deployment | Vercel |

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx              # Public homepage (event grid + hero)
â”‚   â”œâ”€â”€ layout.tsx            # Root layout
â”‚   â”œâ”€â”€ globals.css           # Global styles
â”‚   â”œâ”€â”€ admin/                # Admin dashboard pages
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ auth/             # NextAuth route handler
â”‚       â”œâ”€â”€ events/           # Public event listing API
â”‚       â”œâ”€â”€ tickets/          # Ticket lead submission API
â”‚       â”œâ”€â”€ scrape/           # Cron-triggered scrape endpoint
â”‚       â””â”€â”€ admin/            # Admin-only API routes
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ index.ts              # Manual scrape entry point
â”‚   â”œâ”€â”€ daemon.ts             # Long-running background scraper
â”‚   â”œâ”€â”€ normalizer.ts         # Transforms raw API data â†’ DB schema
â”‚   â”œâ”€â”€ statusEngine.ts       # Upserts events, manages lifecycle
â”‚   â”œâ”€â”€ clear-seed.ts         # Clears + re-seeds the database
â”‚   â””â”€â”€ sources/
â”‚       â”œâ”€â”€ ticketmaster.ts   # Ticketmaster API scraper
â”‚       â”œâ”€â”€ eventbrite.ts     # Eventbrite scraper
â”‚       â”œâ”€â”€ humanitix.ts      # Humanitix scraper
â”‚       â”œâ”€â”€ meetup.ts         # Meetup scraper
â”‚       â””â”€â”€ predicthq.ts      # PredictHQ scraper
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ Event.ts              # Event Mongoose model
â”‚   â”œâ”€â”€ TicketLead.ts         # Ticket lead Mongoose model
â”‚   â””â”€â”€ User.ts               # User Mongoose model
â”‚
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ db.ts                 # MongoDB connection helper
â”‚
â”œâ”€â”€ components/               # Shared React components
â”œâ”€â”€ vercel.json               # Vercel Cron configuration
â””â”€â”€ .env.example              # Environment variable template
```

---

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+
- A [MongoDB Atlas](https://cloud.mongodb.com/) cluster
- A [Google Cloud](https://console.cloud.google.com/) project with OAuth 2.0 credentials
- A [Ticketmaster Developer](https://developer.ticketmaster.com/) API key

### 1. Clone & Install

```bash
git clone <your-repo-url>
cd eventscape
npm install
```

### 2. Configure Environment Variables

Copy the example file and fill in your credentials:

```bash
cp .env.example .env.local
```

```env
# .env.local

MONGODB_URI=mongodb+srv://your_user:your_password@cluster.mongodb.net/eventdiscovery?retryWrites=true&w=majority

GOOGLE_CLIENT_ID=your_google_client_id
GOOGLE_CLIENT_SECRET=your_google_client_secret

NEXTAUTH_SECRET=change_me_to_a_random_32char_string
NEXTAUTH_URL=http://localhost:3000

TICKETMASTER_API_KEY=your_ticketmaster_api_key

# Optional: Protects the /api/scrape endpoint from unauthorised access
CRON_SECRET=your_random_secret_string
```

> **Tip:** Generate `NEXTAUTH_SECRET` with `openssl rand -base64 32`

### 3. Run in Development

```bash
npm run dev
```

This concurrently starts:
- **Next.js** dev server at `http://localhost:3000`
- **Scraper daemon** that fetches events in the background every 6 hours

---

## ğŸ“œ Available Scripts

| Command | Description |
|---|---|
| `npm run dev` | Start Next.js + scraper daemon together |
| `npm run dev:web` | Start Next.js only (no scraper) |
| `npm run build` | Build for production |
| `npm run start` | Start the production server |
| `npm run scrape` | Run a one-off scrape manually |
| `npm run scrape:daemon` | Run the background scraper daemon only |
| `npm run seed` | Seed the database with initial data |
| `npm run db:clear` | Clear all existing events and re-seed |

---

## ğŸ”Œ API Routes

| Method | Route | Description | Auth |
|---|---|---|---|
| `GET` | `/api/events` | List all events | Public |
| `POST` | `/api/tickets` | Submit a ticket lead | Public |
| `GET` | `/api/scrape` | Trigger a scrape run | Cron / Bearer token |
| `GET/POST` | `/api/auth/...` | NextAuth OAuth handlers | â€” |
| `*` | `/api/admin/...` | Admin management routes | Admin only |

### Triggering a Manual Scrape

```bash
curl -H "Authorization: Bearer your_cron_secret" http://localhost:3000/api/scrape
```

---

## âš™ï¸ How the Scraper Works

1. **Sources** â€” Each file in `scraper/sources/` fetches events from a specific API and returns a `RawEvent[]` array.
2. **Normalizer** â€” `normalizer.ts` maps each raw event into a standardised schema.
3. **Status Engine** â€” `statusEngine.ts` upserts events into MongoDB, updating existing records and inserting new ones while preserving data integrity.
4. **Daemon** â€” `daemon.ts` runs the pipeline on a configurable interval (default: every 6 hours).
5. **Cron Endpoint** â€” On Vercel, `vercel.json` triggers `GET /api/scrape` every 6 hours automatically.

---

## â˜ï¸ Deploying to Vercel

1. Push your code to GitHub and import the repo in [Vercel](https://vercel.com/).
2. Add all environment variables from `.env.local` in the Vercel project settings.
3. The `vercel.json` cron job automatically calls `/api/scrape` every 6 hours.

```json
{
  "crons": [
    {
      "path": "/api/scrape",
      "schedule": "0 */6 * * *"
    }
  ]
}
```

> **Note:** Vercel Cron sends requests with the `Authorization: Bearer $CRON_SECRET` header. Make sure to set `CRON_SECRET` in your Vercel environment variables.

---

## ğŸ” Authentication & Admin Access

Authentication is handled by **NextAuth v5** with Google OAuth. Users who sign in with an authorised Google account are granted admin privileges based on their email or a role field stored in the `User` model.

To set up Google OAuth:
1. Go to [Google Cloud Console](https://console.cloud.google.com/) â†’ APIs & Services â†’ Credentials.
2. Create an **OAuth 2.0 Client ID** (Web application).
3. Add `http://localhost:3000/api/auth/callback/google` as an authorised redirect URI.
4. Copy the Client ID and Secret into your `.env.local`.

---

## ğŸ“„ License

MIT â€” feel free to use and adapt this project.
